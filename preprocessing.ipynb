{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amy\\Google Drive\\CSC575\\lisa\\documents\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\Amy\\Google Drive\\CSC575\\lisa\\documents\n",
    "# this is where I initially placed the original 14 files, but it now contains the final 6004 documents\n",
    "# the original 14 files are now placed in the original_dataset folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 14 files downloaded from the Glasgow Respository contain 6004 documents total, so the 14 files need to be processed and separated into the 6004 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filelist=[]\n",
    "for root,dirs,files in os.walk('C:\\Users\\Amy\\Google Drive\\CSC575\\lisa\\documents'):\n",
    "    for name in files:\n",
    "        filelist.append(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Amy\\\\Google Drive\\\\CSC575\\\\lisa\\\\documents\\\\LISA0.001',\n",
       " 'C:\\\\Users\\\\Amy\\\\Google Drive\\\\CSC575\\\\lisa\\\\documents\\\\LISA0.501',\n",
       " 'C:\\\\Users\\\\Amy\\\\Google Drive\\\\CSC575\\\\lisa\\\\documents\\\\LISA1.001',\n",
       " 'C:\\\\Users\\\\Amy\\\\Google Drive\\\\CSC575\\\\lisa\\\\documents\\\\LISA1.501',\n",
       " 'C:\\\\Users\\\\Amy\\\\Google Drive\\\\CSC575\\\\lisa\\\\documents\\\\LISA2.001',\n",
       " 'C:\\\\Users\\\\Amy\\\\Google Drive\\\\CSC575\\\\lisa\\\\documents\\\\LISA2.501',\n",
       " 'C:\\\\Users\\\\Amy\\\\Google Drive\\\\CSC575\\\\lisa\\\\documents\\\\LISA3.001',\n",
       " 'C:\\\\Users\\\\Amy\\\\Google Drive\\\\CSC575\\\\lisa\\\\documents\\\\LISA3.501',\n",
       " 'C:\\\\Users\\\\Amy\\\\Google Drive\\\\CSC575\\\\lisa\\\\documents\\\\LISA4.001',\n",
       " 'C:\\\\Users\\\\Amy\\\\Google Drive\\\\CSC575\\\\lisa\\\\documents\\\\LISA4.501',\n",
       " 'C:\\\\Users\\\\Amy\\\\Google Drive\\\\CSC575\\\\lisa\\\\documents\\\\LISA5.001',\n",
       " 'C:\\\\Users\\\\Amy\\\\Google Drive\\\\CSC575\\\\lisa\\\\documents\\\\LISA5.501',\n",
       " 'C:\\\\Users\\\\Amy\\\\Google Drive\\\\CSC575\\\\lisa\\\\documents\\\\LISA5.627',\n",
       " 'C:\\\\Users\\\\Amy\\\\Google Drive\\\\CSC575\\\\lisa\\\\documents\\\\LISA5.850']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doclist=[]\n",
    "for fil in filelist:\n",
    "    infile=open(fil)\n",
    "    content=infile.read()\n",
    "    infile.close()\n",
    "    strip=content.strip('Document')\n",
    "    docs=strip.split('********************************************\\nDocument ') # split the file into documents \n",
    "    doclist+=docs\n",
    "   \n",
    "cleandocs=[]\n",
    "for doc in doclist:\n",
    "    doc = doc.strip() # remove the blank spaces at the beginning of string\n",
    "    cleandocs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write each item (document) in the list to a write\n",
    "docnum=0\n",
    "for cleandoc in cleandocs:\n",
    "    docnum+=1\n",
    "    f=open('{}.txt'.format(docnum),'w')\n",
    "    f.write(cleandoc)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# due to original formatting, there is a document number on the first line of each document.\n",
    "# Remove first line of each document so that document only contains document content. \n",
    "for i in range(1,docnum+1):\n",
    "    lines = open('{}.txt'.format(i)).readlines()\n",
    "    open('{}.txt'.format(i), 'w').writelines(lines[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each document also has a title and a body. Create a dictionary with the doc num as the key, and the title and document as separate values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docdict={}\n",
    "for i in range(1,1501):\n",
    "    lines = open('{}.txt'.format(i))\n",
    "    content=lines.read()\n",
    "    docdict[i]=content.split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1501,5001):\n",
    "    lines = open('{}.txt'.format(i))\n",
    "    content=lines.read()\n",
    "    docdict[i]=content.split('\\n     \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(5001,6005):\n",
    "    lines = open('{}.txt'.format(i))\n",
    "    content=lines.read()\n",
    "    docdict[i]=content.split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6004"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE ORGANIZATION AND USE OF INFORMATION: CONTRIBUTIONS OF INFORMATION SCIENCE,\\nCOMPUTATIONAL LINGUISTICS AND ARTIFICIAL INTELLIGENCE.',\n",
       " 'REVIEWS THE WAYS IN WHICH NEW DEVELOPMENTS IN COMPUTER-BASED METHODS FOR\\nWORKING WITH CONCEPTS OF INFORMATION, KNOWLEDGE AND LANGUAGE CAN IMPROVE THE\\nUNDERSTANDING OF HOW PEOPLE ORGANISE AND USE INFORMATION.\\n********************************************']"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docdict[6004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(docdict, open( \"docdict.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There is one file containing a list of 35 queries, and another containing the relevant docs for each query. Create dictionaries for them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amy\\Google Drive\\CSC575\\lisa\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\Amy\\Google Drive\\CSC575\\lisa\n",
    "# this is where I initially placed the original query and relevance files, but they have been moved to the 'original_dataset' folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infile=open('LISA.QUE')\n",
    "content=infile.read()\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content2=content.split('#\\n') # split query list into individual queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content3=content2[:-1] # last item is an empty item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write each query to a new file\n",
    "querynum=0\n",
    "for query in content3:\n",
    "    querynum+=1\n",
    "    f=open('{}.txt'.format(querynum),'w')\n",
    "    f.write(query)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove first line (contains query #)\n",
    "for i in range(1,querynum+1):\n",
    "    lines = open('{}.txt'.format(i)).readlines()\n",
    "    fout=open('{}.txt'.format(i), 'w')\n",
    "    fout.writelines(lines[1:])\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create query dictionary\n",
    "querydict={}\n",
    "for i in range(1,querynum+1):\n",
    "    infile=open('{}.txt'.format(i))\n",
    "    content=infile.read()\n",
    "    querydict[i]=content\n",
    "    infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infile=open('LISA.REL')\n",
    "content=infile.read()\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content2=content.split('Query ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content3=content2[1:] # remove first item because it's empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write each relevance info to a new file\n",
    "relnum=0\n",
    "for rel in content3:\n",
    "    relnum+=1\n",
    "    f=open('rel{}.txt'.format(relnum),'w')\n",
    "    f.write(rel)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove lines 1-2 (contains query # and # of relevant docs)\n",
    "for i in range(1,relnum+1):\n",
    "    rel = open('rel{}.txt'.format(i))\n",
    "    lines=rel.readlines()\n",
    "    fout=open('rel{}.txt'.format(i), 'w')\n",
    "    fout.writelines(lines[2:])\n",
    "    rel.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove ' -1' \n",
    "for i in range(1,relnum+1):\n",
    "    rel = open('rel{}.txt'.format(i))\n",
    "    content=rel.read()\n",
    "    content2=content.replace(' -1','')\n",
    "    fout=open('rel{}.txt'.format(i), 'w')\n",
    "    fout.write(content2)\n",
    "    rel.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create relevance dictionary\n",
    "reldict={}\n",
    "for i in range(1,relnum+1):\n",
    "    infile=open('rel{}.txt'.format(i))\n",
    "    content=infile.read()\n",
    "    content2=content.split()\n",
    "    reldict[i]=content2\n",
    "    infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['934', '1433', '2898', '2899', '2948', '2950', '3386', '3396', '4284', '4365']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reldict[31]# for query 31, this is the list of relevent documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(reldict, open( \"relevance.p\", \"wb\" ) )\n",
    "pickle.dump(querydict, open( \"queries.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
